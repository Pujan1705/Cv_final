{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"****Difference between traditional CV and DL based CV****","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"Traditional computer vision (CV) and deep learning (DL) based CV are two different approaches to solving problems in computer vision.\n\nTraditional CV relies on manually designing and engineering features and algorithms to extract useful information from images or videos. This includes techniques such as edge detection, corner detection, template matching, and feature extraction using SIFT or SURF. These features are then fed into classifiers or other algorithms to perform tasks such as object detection, tracking, and recognition.\n\nOn the other hand, DL based CV uses neural networks to automatically learn useful features and representations from images or videos. These networks are typically trained on large datasets and can learn to recognize patterns and features that are difficult to engineer by hand. DL based CV includes techniques such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and generative adversarial networks (GANs).\n\nDL based CV has achieved state-of-the-art performance on many computer vision tasks, including image classification, object detection, segmentation, and more. It has also enabled new applications such as image style transfer, super-resolution, and image generation.\n\nOverall, traditional CV and DL based CV are complementary approaches, and the choice of which to use depends on the specific problem and available data. Traditional CV is still useful in scenarios where the available data is limited or the task is simple, while DL based CV is generally more effective for complex tasks with large amounts of data.\n","metadata":{}},{"cell_type":"markdown","source":"Traditional Computer Vision\nRelies on basic image processing techniques\nHandcrafted feature extraction/Mathematical feature extraction\nClassic Machine Learning techniques e.g. Image classification using SVM, Naïve Bayes, Logistic Regression, etc. Image Segmentation using K-Means \n","metadata":{}},{"cell_type":"markdown","source":"Data Driven approach (requires image data)\nFeature extraction and pattern recognition is learnt during the training process\nTraining neural networks on a dataset of images for the desired task (classification, segmentation, object detection, etc)\n","metadata":{}},{"cell_type":"markdown","source":"**Typical tasks in CV**\n","metadata":{}},{"cell_type":"markdown","source":"Computer vision (CV) is a rapidly growing field with a wide range of applications. Here are some typical tasks in CV:\n\nImage classification: This involves assigning a label or category to an image, such as \"dog,\" \"cat,\" or \"car.\"\n\nObject detection: This involves identifying the location and size of objects within an image or video, and classifying them into specific categories.\n\nObject tracking: This involves following an object over time in a video, even as it moves or changes in appearance.\n\nImage segmentation: This involves dividing an image into different regions or segments based on their properties, such as color, texture, or intensity.\n\nImage registration: This involves aligning two or more images that may be taken from different viewpoints or at different times.\n\nImage restoration: This involves removing noise or other artifacts from an image to improve its quality.\n\nFace recognition: This involves identifying individuals based on their facial features, often used in security systems or social media applications.\n\n3D reconstruction: This involves creating a 3D model of an object or scene from multiple 2D images or video frames.\n\nOptical character recognition (OCR): This involves recognizing text within an image and converting it into a machine-readable format.\n\nImage synthesis: This involves generating new images or videos based on existing data or models, such as image style transfer or video prediction.\n\nThese are just a few examples of the many tasks in CV, and new applications are being developed all the time.","metadata":{}},{"cell_type":"markdown","source":"**CV pipeline**","metadata":{}},{"cell_type":"markdown","source":"A computer vision (CV) pipeline is a series of steps or processes used to perform an analysis or solve a problem in CV. Here's a typical CV pipeline:\n\nData acquisition: This involves collecting or creating a dataset of images or videos to be used in the analysis.\n\nPreprocessing: This involves applying techniques such as resizing, cropping, and normalization to prepare the images or videos for analysis.\n\nFeature extraction: This involves using traditional CV techniques or deep learning models to extract meaningful features or representations from the images or videos.\n\nTraining: This involves using the extracted features or representations to train a model or algorithm to perform a specific task, such as image classification, object detection, or segmentation.\n\nEvaluation: This involves testing the trained model on a separate set of data to measure its performance and assess its accuracy.\n\nDeployment: This involves integrating the trained model into a larger system or application, such as a self-driving car or a medical diagnosis tool.\n\nThe specific steps in a CV pipeline can vary depending on the problem and the available data, but this general pipeline provides a framework for organizing and executing the various tasks involved in CV analysis. It's important to note that the pipeline is an iterative process, with each step potentially feeding back into earlier steps to improve the overall performance of the system.","metadata":{}},{"cell_type":"markdown","source":"**Digital image as a 2D function and image representation**\n","metadata":{}},{"cell_type":"markdown","source":"An image may be defined as a two-dimensional function, f(x, y), where x and y are spatial (plane) coordinates, and the amplitude of f at any pair of coordinates (x, y) is called the intensity or gray level of the image at that point.\n\nIn an 8-bit image each pixel occupies exactly one byte. This means each pixel has 256 possible numerical values, from 0 to 255. Therefore, the color palette for an 8-bit image normally contains 256 entries, defining color 0 through color 255.","metadata":{}},{"cell_type":"markdown","source":"**Countour Detection**","metadata":{}},{"cell_type":"markdown","source":"Contours are defined as the line joining all the points along the boundary of an image that are having the same intensity. Contours come handy in shape analysis, finding the size of the object of interest, and object detection.\n","metadata":{}},{"cell_type":"markdown","source":"**Edge Detection**","metadata":{}},{"cell_type":"markdown","source":"Edge detection is a fundamental image processing technique that aims to identify the boundaries between objects and the background in an image. The edges can be defined as areas of rapid intensity change in an image, and they can be classified into three categories: step edges, ramp edges, and roof edges.\n\nEdge detection is typically performed using various mathematical techniques such as convolution, gradient-based methods, and Laplacian-based methods. One of the most widely used edge detection algorithms is the Canny edge detector, which uses a multi-stage process to identify edges in an image. The first stage involves applying a Gaussian filter to smooth the image and reduce noise. The second stage involves computing the gradient of the image to identify regions of rapid intensity change. Finally, non-maximum suppression and hysteresis thresholding are used to remove false edges and detect true edges.\n\nEdge detection is used in many applications, such as object recognition, image segmentation, and computer vision. It is also used in various fields, including medical imaging, robotics, and autonomous vehicles.","metadata":{}},{"cell_type":"markdown","source":"**Code for Edge detection**","metadata":{}},{"cell_type":"code","source":"import cv2\n\n# read input image\nimg = cv2.imread('input_image.jpg', 0)\n\n# apply Gaussian filter to smooth the image\nblur = cv2.GaussianBlur(img, (3, 3), 0)\n\n# compute gradient using Sobel operator\ngrad_x = cv2.Sobel(blur, cv2.CV_64F, 1, 0, ksize=3)\ngrad_y = cv2.Sobel(blur, cv2.CV_64F, 0, 1, ksize=3)\ngrad = cv2.addWeighted(grad_x, 0.5, grad_y, 0.5, 0)\n\n# apply non-maximum suppression to remove false edges\nnms = cv2.Canny(grad, 100, 200, apertureSize=3, L2gradient=True)\n\n# display output image\ncv2.imshow('Output Image', nms)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-18T05:02:14.036562Z","iopub.execute_input":"2023-04-18T05:02:14.037008Z","iopub.status.idle":"2023-04-18T05:02:14.368963Z","shell.execute_reply.started":"2023-04-18T05:02:14.036969Z","shell.execute_reply":"2023-04-18T05:02:14.367297Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/2062929580.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# apply Gaussian filter to smooth the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mblur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGaussianBlur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# compute gradient using Sobel operator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.5.4) /tmp/pip-req-build-jpmv6t9_/opencv/modules/imgproc/src/smooth.dispatch.cpp:617: error: (-215:Assertion failed) !_src.empty() in function 'GaussianBlur'\n"],"ename":"error","evalue":"OpenCV(4.5.4) /tmp/pip-req-build-jpmv6t9_/opencv/modules/imgproc/src/smooth.dispatch.cpp:617: error: (-215:Assertion failed) !_src.empty() in function 'GaussianBlur'\n","output_type":"error"}]},{"cell_type":"markdown","source":"n this code, we first read the input image using the OpenCV library. Then we apply a Gaussian filter to smooth the image and reduce noise. Next, we compute the gradient of the image using the Sobel operator. We then apply non-maximum suppression to remove false edges and finally display the output image using the imshow() function. The waitKey() and destroyAllWindows() functions are used to wait for a key press and close the window, respectively.\n\nNote that the Canny() function in OpenCV automatically performs Gaussian blurring, gradient computation, non-maximum suppression, and hysteresis thresholding, so we don't need to perform those steps manually. We just need to provide the input image and appropriate threshold values for the Canny function to perform edge detection.","metadata":{}},{"cell_type":"markdown","source":"**Canny detection**","metadata":{}},{"cell_type":"markdown","source":"The Canny edge detection algorithm is a popular method for detecting edges in an image. It was developed by John F. Canny in 1986 and is still widely used today due to its accuracy and simplicity. The algorithm consists of several steps, including smoothing the image, computing the gradient magnitude and direction, non-maximum suppression, and hysteresis thresholding.\n\nHere is the step-by-step process of the Canny edge detection algorithm:\n\nApply Gaussian blur to the input image to remove noise.\nCompute the gradient magnitude and direction using the Sobel operator.\nPerform non-maximum suppression to thin the edges by suppressing non-maximum pixels along the direction of the gradient.\nApply hysteresis thresholding to remove weak edges and connect strong edges. This involves setting two threshold values: a low threshold and a high threshold. Any edge with a magnitude below the low threshold is suppressed, while edges with a magnitude above the high threshold are retained. Edges with a magnitude between the low and high thresholds are retained only if they are connected to strong edges.\nFinally, display the detected edges on the output image.","metadata":{}},{"cell_type":"code","source":"import cv2\n\n# Read the input image\nimg = cv2.imread('input_image.jpg', 0)\n\n# Apply Gaussian blur\nimg_blur = cv2.GaussianBlur(img, (5, 5), 0)\n\n# Compute gradient magnitude and direction\ngrad_x = cv2.Sobel(img_blur, cv2.CV_64F, 1, 0, ksize=3)\ngrad_y = cv2.Sobel(img_blur, cv2.CV_64F, 0, 1, ksize=3)\ngrad_mag = cv2.magnitude(grad_x, grad_y)\ngrad_dir = cv2.phase(grad_x, grad_y, angleInDegrees=True)\n\n# Perform non-maximum suppression\nnms = cv2.Canny(grad_mag, 100, 200, apertureSize=3, L2gradient=True)\n\n# Display the output image\ncv2.imshow('Canny Edge Detection', nms)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-18T05:03:54.977839Z","iopub.execute_input":"2023-04-18T05:03:54.978310Z","iopub.status.idle":"2023-04-18T05:03:55.000256Z","shell.execute_reply.started":"2023-04-18T05:03:54.978274Z","shell.execute_reply":"2023-04-18T05:03:54.998422Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/4091087173.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Apply Gaussian blur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mimg_blur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGaussianBlur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Compute gradient magnitude and direction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.5.4) /tmp/pip-req-build-jpmv6t9_/opencv/modules/imgproc/src/smooth.dispatch.cpp:617: error: (-215:Assertion failed) !_src.empty() in function 'GaussianBlur'\n"],"ename":"error","evalue":"OpenCV(4.5.4) /tmp/pip-req-build-jpmv6t9_/opencv/modules/imgproc/src/smooth.dispatch.cpp:617: error: (-215:Assertion failed) !_src.empty() in function 'GaussianBlur'\n","output_type":"error"}]},{"cell_type":"markdown","source":"**Line Detection**","metadata":{}},{"cell_type":"markdown","source":"Line detection is a common task in computer vision and image processing. The Hough transform is a popular technique for detecting lines in an image. It works by representing lines in the image space as points in the Hough space, where each point corresponds to a line in the image space. The Hough transform can detect lines of any orientation and can handle noisy images.\n\nHere are the steps involved in the Hough transform for line detection:\n\nConvert the input image to a binary image using thresholding or edge detection.\nInitialize an accumulator matrix (Hough space) with zeros.\nFor each non-zero pixel in the binary image, compute the corresponding lines in the image space and increment the corresponding cells in the accumulator matrix.\nThreshold the accumulator matrix to extract the lines with the highest vote counts.\nConvert the lines from Hough space back to the image space.\nHere is a Python code for line detection using Hough transform in OpenCV:\n\npython\nCopy code\nimport cv2\nimport numpy as np\n\n# Read the input image\nimg = cv2.imread('input_image.jpg')\n\n# Convert the input image to grayscale\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Apply Canny edge detection\nedges = cv2.Canny(gray, 50, 150, apertureSize=3)\n\n# Apply Hough transform for line detection\nlines = cv2.HoughLines(edges, 1, np.pi/180, 100)\n\n# Draw the detected lines on the input image\nif lines is not None:\n    for line in lines:\n        rho, theta = line[0]\n        a = np.cos(theta)\n        b = np.sin(theta)\n        x0 = a * rho\n        y0 = b * rho\n        x1 = int(x0 + 1000*(-b))\n        y1 = int(y0 + 1000*(a))\n        x2 = int(x0 - 1000*(-b))\n        y2 = int(y0 - 1000*(a))\n        cv2.line(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n\n# Display the output image\ncv2.imshow('Line Detection', img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\nIn this code, we first read the input image using the imread() function of OpenCV. Then we convert the image to grayscale using the cvtColor() function, and apply Canny edge detection using the Canny() function. Next, we use the HoughLines() function to detect lines in the image. The first argument of the function is the input image, the second argument is the distance resolution in pixels, the third argument is the angle resolution in radians, and the fourth argument is the threshold for the minimum number of votes required to accept a line. The function returns an array of lines in polar coordinates (rho and theta).\n\nFinally, we iterate over the lines and draw them on the input image using the line() function of OpenCV. The first argument of the function is the input image, the second and third arguments are the coordinates of the start and end points of the line, the fourth argument is the color of the line, and the fifth argument is the thickness of the line. We then display the output image using the imshow() function, and wait for a key press using waitKey(), and then close the window using destroyAllWindows().\n\n","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\n# Read the input image\nimg = cv2.imread('input_image.jpg')\n\n# Convert the input image to grayscale\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Apply Canny edge detection\nedges = cv2.Canny(gray, 50, 150, apertureSize=3)\n\n# Apply Hough transform for line detection\nlines = cv2.HoughLines(edges, 1, np.pi/180, 100)\n\n# Draw the detected lines on the input image\nif lines is not None:\n    for line in lines:\n        rho, theta = line[0]\n        a = np.cos(theta)\n        b = np.sin(theta)\n        x0 = a * rho\n        y0 = b * rho\n        x1 = int(x0 + 1000*(-b))\n        y1 = int(y0 + 1000*(a))\n        x2 = int(x0 - 1000*(-b))\n        y2 = int(y0 - 1000*(a))\n        cv2.line(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n\n# Display the output image\ncv2.imshow('Line Detection', img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-18T05:06:09.193140Z","iopub.execute_input":"2023-04-18T05:06:09.194416Z","iopub.status.idle":"2023-04-18T05:06:09.221675Z","shell.execute_reply.started":"2023-04-18T05:06:09.194359Z","shell.execute_reply":"2023-04-18T05:06:09.220112Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/2123934609.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Convert the input image to grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Apply Canny edge detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.5.4) /tmp/pip-req-build-jpmv6t9_/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"],"ename":"error","evalue":"OpenCV(4.5.4) /tmp/pip-req-build-jpmv6t9_/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n","output_type":"error"}]},{"cell_type":"markdown","source":"**Hough transformation**","metadata":{}},{"cell_type":"markdown","source":"The Hough transform is a feature extraction technique used in computer vision and image analysis for detecting shapes in an image. The most common application of the Hough transform is to detect lines, but it can also be used to detect other shapes such as circles and ellipses.\n\nThe Hough transform works by converting image points to a parameter space representation, where each point in the parameter space represents a possible shape that could pass through a set of image points. In the case of line detection, the Hough transform converts each image point to a line in parameter space, represented by its slope and intercept.\n\nThe Hough transform algorithm works as follows:\n\nEdge detection: Detect edges in the input image using an edge detection algorithm such as Canny edge detector.\n\nParameter space creation: For each edge point in the input image, compute a set of lines in parameter space that could pass through the point.\n\nVoting: For each line in parameter space that passes through one or more edge points, increment a vote counter for that line.\n\nThresholding: Set a threshold for the minimum number of votes required for a line to be considered a valid detection.\n\nLine extraction: Extract the lines with the highest vote counts from the parameter space.\n\nReturn: The output of the algorithm is a set of detected lines in the input image.","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\n# Read the input image\nimg = cv2.imread('input_image.jpg')\n\n# Convert the input image to grayscale\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Apply Canny edge detection\nedges = cv2.Canny(gray, 50, 150, apertureSize=3)\n\n# Apply Hough transform for line detection\nlines = cv2.HoughLines(edges, 1, np.pi/180, 200)\n\n# Draw the detected lines on the input image\nif lines is not None:\n    for line in lines:\n        rho, theta = line[0]\n        a = np.cos(theta)\n        b = np.sin(theta)\n        x0 = a * rho\n        y0 = b * rho\n        x1 = int(x0 + 1000*(-b))\n        y1 = int(y0 + 1000*(a))\n        x2 = int(x0 - 1000*(-b))\n        y2 = int(y0 - 1000*(a))\n        cv2.line(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n\n# Display the output image\ncv2.imshow('Line Detection', img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-18T05:10:41.716472Z","iopub.execute_input":"2023-04-18T05:10:41.716915Z","iopub.status.idle":"2023-04-18T05:10:41.741159Z","shell.execute_reply.started":"2023-04-18T05:10:41.716876Z","shell.execute_reply":"2023-04-18T05:10:41.739618Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/1963006123.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Convert the input image to grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Apply Canny edge detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.5.4) /tmp/pip-req-build-jpmv6t9_/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"],"ename":"error","evalue":"OpenCV(4.5.4) /tmp/pip-req-build-jpmv6t9_/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n","output_type":"error"}]},{"cell_type":"markdown","source":"**Image Features**","metadata":{}},{"cell_type":"markdown","source":"Image features, also known as keypoints, are points in an image that represent distinctive local features that can be used for tasks such as image matching, object recognition, and 3D reconstruction. These keypoints are typically robust to changes in lighting, viewpoint, and scale, making them useful for a variety of computer vision tasks.\n\nThere are several types of image features that are commonly used in computer vision:\n\nCorner features: Corner features are points in an image where the gradient changes abruptly in multiple directions. Examples of corner features include Harris corners and FAST corners.\n\nBlob features: Blob features are regions in an image where the intensity changes gradually in all directions. Examples of blob features include Difference of Gaussians (DoG) and Scale-Invariant Feature Transform (SIFT).\n\nEdge features: Edge features are locations in an image where there is a significant change in intensity in a single direction. Examples of edge features include Canny edges and Sobel edges.\n\nLine features: Line features are straight or curved line segments in an image. Examples of line features include Hough transform lines and LSD lines.\n\nRegion features: Region features are groups of pixels that form a distinct region in an image. Examples of region features include Histogram of Oriented Gradients (HOG) and Local Binary Patterns (LBP).\n\nOnce image features are detected, they can be described using feature descriptors, which are vectors that summarize the local image information around each feature point. Common feature descriptors include SIFT, SURF, and ORB.\n\nImage features and their descriptors are often used in combination with machine learning algorithms such as support vector machines (SVM) and random forests to perform tasks such as object detection, image classification, and image retrieval.","metadata":{}},{"cell_type":"markdown","source":"**SIFT algorithm**","metadata":{}},{"cell_type":"markdown","source":"SIFT (Scale-Invariant Feature Transform) is a widely used computer vision algorithm for detecting and describing local features in images. It was developed by David Lowe in 1999 and has become one of the most popular algorithms for feature detection and matching due to its robustness to changes in scale, orientation, and lighting.\n\nThe SIFT algorithm works as follows:\n\nScale-space extrema detection: The first step in the SIFT algorithm is to find potential feature locations at multiple scales in the image. This is achieved by computing the difference of Gaussian (DoG) pyramid, which is a series of images obtained by convolving the input image with Gaussian filters of increasing standard deviation, and subtracting adjacent levels of the pyramid.\n\nKeypoint localization: Next, potential keypoints are selected from the scale-space extrema by comparing them to their neighboring pixels in scale and space. Keypoints that are poorly localized or have low contrast are discarded.\n\nOrientation assignment: For each keypoint, a dominant orientation is assigned based on the gradient orientations in the local neighborhood around the keypoint. This makes the feature descriptor rotationally invariant.\n\nDescriptor generation: Finally, a feature descriptor is generated for each keypoint by computing the gradient magnitude and orientation in a region around the keypoint, and building a histogram of gradient orientations weighted by the gradient magnitudes. The resulting histogram is normalized and concatenated to form a feature vector.","metadata":{}},{"cell_type":"code","source":"import cv2\n\n# Load two images\nimg1 = cv2.imread('image1.jpg')\nimg2 = cv2.imread('image2.jpg')\n\n# Convert images to grayscale\ngray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\ngray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n\n# Initialize SIFT detector\nsift = cv2.xfeatures2d.SIFT_create()\n\n# Detect and compute keypoints and descriptors for both images\nkp1, desc1 = sift.detectAndCompute(gray1, None)\nkp2, desc2 = sift.detectAndCompute(gray2, None)\n\n# Create a BFMatcher object for matching keypoints\nbf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n\n# Match keypoints in both images\nmatches = bf.match(desc1, desc2)\n\n# Sort matches by distance\nmatches = sorted(matches, key = lambda x:x.distance)\n\n# Draw the top N matches between the images\nimg_matches = cv2.drawMatches(img1, kp1, img2, kp2, matches[:10], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n\n# Display the output image\ncv2.imshow('SIFT Matches', img_matches)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-18T05:19:31.657995Z","iopub.execute_input":"2023-04-18T05:19:31.658406Z","iopub.status.idle":"2023-04-18T05:19:31.681535Z","shell.execute_reply.started":"2023-04-18T05:19:31.658372Z","shell.execute_reply":"2023-04-18T05:19:31.679630Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3855102164.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Convert images to grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgray1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mgray2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.5.4) /tmp/pip-req-build-jpmv6t9_/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"],"ename":"error","evalue":"OpenCV(4.5.4) /tmp/pip-req-build-jpmv6t9_/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n","output_type":"error"}]},{"cell_type":"markdown","source":"**Affine transformation**","metadata":{}},{"cell_type":"markdown","source":"Affine transformation is a type of transformation that preserves the parallelism of lines and angles between them. It is a linear mapping that can be represented by a 2x3 matrix, which transforms a point (x, y) to a new point (x', y') in the form:\n\nx' = a * x + b * y + c\ny' = d * x + e * y + f\n\nwhere a, b, d, e are the scale factors and c, f are the translation factors. This transformation can also include shear and rotation, which are additional parameters in the matrix.\n\nAffine transformations can be used in image processing for a variety of tasks, such as image scaling, rotation, translation, and skew correction. It is also a fundamental tool in computer vision for geometric transformations of images, object detection, and recognition.","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\n# Load image\nimg = cv2.imread('image.jpg')\n\n# Set source and destination points\npts1 = np.float32([[50,50],[200,50],[50,200]])\npts2 = np.float32([[10,100],[200,50],[100,250]])\n\n# Compute affine matrix using the source and destination points\nM = cv2.getAffineTransform(pts1, pts2)\n\n# Apply affine transformation to the image\nimg_affine = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n\n# Display the original and transformed images\ncv2.imshow('Original Image', img)\ncv2.imshow('Affine Transformed Image', img_affine)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-18T05:22:07.826851Z","iopub.execute_input":"2023-04-18T05:22:07.828089Z","iopub.status.idle":"2023-04-18T05:22:07.857090Z","shell.execute_reply.started":"2023-04-18T05:22:07.828038Z","shell.execute_reply":"2023-04-18T05:22:07.855725Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/4022842293.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Apply affine transformation to the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mimg_affine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarpAffine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Display the original and transformed images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"],"ename":"AttributeError","evalue":"'NoneType' object has no attribute 'shape'","output_type":"error"}]},{"cell_type":"markdown","source":"**Intro to CNN**","metadata":{}},{"cell_type":"markdown","source":"A Convolutional Neural Network (CNN) is a type of deep learning algorithm that is commonly used in image recognition and processing tasks. The main idea behind CNNs is to learn the features of images by applying a series of convolutional filters to the input image. These filters slide over the image and perform element-wise multiplication with the corresponding pixel values in each region to produce a feature map.\n\nEach filter is responsible for detecting a specific feature in the image, such as edges, corners, or textures. By stacking multiple convolutional layers, the CNN can learn increasingly complex and abstract features in the input image. The output of the last convolutional layer is then flattened and fed into a fully connected (dense) layer for classification or regression.\n\nThe key components of a CNN include:\n\nConvolutional Layer: Applies a set of filters to the input image to produce a set of feature maps.\n\nPooling Layer: Subsamples the feature maps to reduce their spatial dimensions and extract the most important features.\n\nActivation Function: Introduces non-linearity into the network and enables the model to learn complex patterns.\n\nFully Connected Layer: Takes the flattened output from the convolutional layers and applies a set of weights to produce the final output (classification or regression).","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\n# Create a Sequential model\nmodel = Sequential()\n\n# Add a convolutional layer with 32 filters, a 3x3 kernel, and ReLU activation\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n\n# Add a max pooling layer with a 2x2 pool size\nmodel.add(MaxPooling2D((2, 2)))\n\n# Add another convolutional layer with 64 filters, a 3x3 kernel, and ReLU activation\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\n\n# Add another max pooling layer with a 2x2 pool size\nmodel.add(MaxPooling2D((2, 2)))\n\n# Flatten the output of the previous layer\nmodel.add(Flatten())\n\n# Add a fully connected layer with 128 units and ReLU activation\nmodel.add(Dense(128, activation='relu'))\n\n# Add an output layer with 1 unit and sigmoid activation for binary classification\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model with binary crossentropy loss and Adam optimizer\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Print the model summary\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-18T05:29:04.433179Z","iopub.execute_input":"2023-04-18T05:29:04.433569Z","iopub.status.idle":"2023-04-18T05:29:14.303557Z","shell.execute_reply.started":"2023-04-18T05:29:04.433535Z","shell.execute_reply":"2023-04-18T05:29:14.302042Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 62, 62, 32)        896       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 29, 29, 64)        18496     \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 14, 14, 64)       0         \n 2D)                                                             \n                                                                 \n flatten (Flatten)           (None, 12544)             0         \n                                                                 \n dense (Dense)               (None, 128)               1605760   \n                                                                 \n dense_1 (Dense)             (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 1,625,281\nTrainable params: 1,625,281\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Image augmentation**","metadata":{}},{"cell_type":"markdown","source":"Image augmentation is a technique used in computer vision and deep learning to artificially expand the size of a training dataset by generating new images through various transformations. These transformed images are still representative of the original images but are slightly modified in some way. Image augmentation is used to improve the accuracy and robustness of deep learning models by exposing the model to a larger and more diverse set of training data.\n\nHere are some commonly used image augmentation techniques:\n\nRotation: Rotates the image by a specified angle.\n\nFlipping: Flips the image horizontally or vertically.\n\nScaling: Rescales the image by a specified factor.\n\nTranslation: Translates the image horizontally or vertically.\n\nNoise addition: Adds random noise to the image.\n\nColor jitter: Randomly adjusts the brightness, contrast, and saturation of the image.\n\nShearing: Applies a shearing transformation to the image.\n\nCropping: Crops a portion of the image.\n\nPerspective transformation: Applies a perspective transformation to the image.\n\nElastic transformation: Applies a random deformation to the image.","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\n# Define the image augmentation parameters\ndatagen = ImageDataGenerator(\n        rotation_range=20,           # Rotate the image by 20 degrees\n        width_shift_range=0.2,      # Shift the image horizontally by 20% of its width\n        height_shift_range=0.2,     # Shift the image vertically by 20% of its height\n        zoom_range=0.2,             # Zoom the image by 20%\n        horizontal_flip=True,       # Flip the image horizontally\n        vertical_flip=True)         # Flip the image vertically\n\n# Load the image dataset\ntrain_dataset = datagen.flow_from_directory(\n        'train/',\n        target_size=(64, 64),\n        batch_size=32,\n        class_mode='binary')\n\n# Train the model using the augmented dataset\nmodel.fit(\n        train_dataset,\n        steps_per_epoch=len(train_dataset),\n        epochs=10)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-18T05:33:30.329252Z","iopub.execute_input":"2023-04-18T05:33:30.330372Z","iopub.status.idle":"2023-04-18T05:33:30.661434Z","shell.execute_reply.started":"2023-04-18T05:33:30.330324Z","shell.execute_reply":"2023-04-18T05:33:30.659551Z"},"trusted":true},"execution_count":9,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/582697221.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         class_mode='binary')\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Train the model using the augmented dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1664\u001b[0m             \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1665\u001b[0m             \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1666\u001b[0;31m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m         )\n\u001b[1;32m   1668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train/'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'train/'","output_type":"error"}]},{"cell_type":"markdown","source":"**Dropout\nFeature scaling\nInternal covariate shift\nBatch normalization\nGlobal average pooling\nTransfer learning - AlexNet, VGG16\nPretrained network as classifier\nFine tuning\nModel callbacks\nROC AUC**","metadata":{}},{"cell_type":"markdown","source":"Dropout: Dropout is a regularization technique used in deep learning to prevent overfitting. It randomly drops out some of the neurons during training, forcing the network to learn more robust features.\n\nFeature scaling: Feature scaling is the process of scaling the input features to a common range to improve the performance of the machine learning algorithms. Commonly used scaling techniques include min-max scaling, z-score normalization, and log normalization.\n\nInternal covariate shift: Internal covariate shift is the change in the distribution of the input data distribution to a deep learning model during training, which can slow down the training process and reduce the accuracy of the model. Batch normalization is a technique used to address internal covariate shift.\n\nBatch normalization: Batch normalization is a technique used in deep learning to improve the stability and convergence speed of a neural network. It normalizes the activations of the previous layer across a mini-batch of inputs, reducing the internal covariate shift.\n\nGlobal average pooling: Global average pooling is a technique used in deep learning to reduce the spatial dimensions of the feature maps by computing the average of each feature map.\n\nTransfer learning - AlexNet, VGG16: Transfer learning is the process of using a pre-trained deep learning model as a starting point for a new task. AlexNet and VGG16 are popular deep learning models that have been pre-trained on large image datasets and can be used as a starting point for image classification tasks.\n\nPretrained network as classifier: A pre-trained network can be used as a classifier by removing the last fully connected layer and replacing it with a new output layer that matches the number of classes in the new task.\n\nFine tuning: Fine-tuning is a technique used in transfer learning where a pre-trained network is further trained on a new task by freezing some of the early layers and training the remaining layers.\n\nModel callbacks: Model callbacks are functions that are called during the training process of a deep learning model. They can be used to implement early stopping, learning rate scheduling, and other custom functionality.\n\nROC AUC: ROC AUC is a performance metric used in binary classification problems to measure the ability of a classifier to distinguish between positive and negative classes. It represents the area under the receiver operating characteristic (ROC) curve.","metadata":{}},{"cell_type":"markdown","source":"****Inception V1\nResNet\nMobileNet\nClass activation maps ****","metadata":{}},{"cell_type":"markdown","source":"Inception V1:\n\nInception V1, also known as GoogLeNet, is a deep convolutional neural network that was designed to improve the efficiency of deep learning models by using a combination of small filters and parallel pooling operations. The Inception module, which consists of multiple parallel convolutional layers with different filter sizes, is the key building block of the network. Inception V1 achieved state-of-the-art performance on the ImageNet classification task in 2014.\n\nResNet:\n\nResNet, short for Residual Network, is a deep neural network architecture that introduces residual connections, also known as skip connections, to overcome the problem of vanishing gradients in very deep networks. The residual connections allow the gradient to flow directly through the network without being affected by intermediate layers, which can significantly improve the accuracy and convergence speed of deep networks. ResNet achieved state-of-the-art performance on the ImageNet classification task in 2015.\n\nMobileNet:\n\nMobileNet is a family of lightweight convolutional neural networks designed for mobile and embedded applications. MobileNet uses depthwise separable convolutions, which separate the spatial and channel-wise convolutions, to significantly reduce the number of parameters and computation required by the network. MobileNet achieves high accuracy on various image classification tasks while being much faster and smaller than traditional deep neural networks.\n\nClass activation maps:\n\nClass activation maps (CAM) is a technique used in convolutional neural networks to visualize the regions of an input image that are most important for a specific class. CAM works by computing the weighted sum of the feature maps of the final convolutional layer, where the weights are learned using the global average pooling of the feature maps and the final fully connected layer. The resulting map can be overlaid on the input image to highlight the regions that contribute most to the classification decision. CAM can be used to explain the decision-making process of a neural network and to identify the salient features of an input image.","metadata":{}},{"cell_type":"markdown","source":"**Visual embeddings\nReverse image search\nFace recognition / identification\nSiamese network**","metadata":{}},{"cell_type":"markdown","source":"Visual embeddings:\n\nVisual embeddings, also known as image embeddings or visual features, are a compact representation of an image that captures its key visual information. Visual embeddings are learned by deep neural networks, such as convolutional neural networks (CNNs), and are often used as a starting point for various computer vision tasks, such as image classification, object detection, and image retrieval.\n\nReverse image search:\n\nReverse image search is a technique used to find similar images to a given image by using the image as a query. Reverse image search works by extracting visual embeddings from the query image and comparing them to the embeddings of a large database of images. The search engine then returns a list of images that are visually similar to the query image. Reverse image search can be used for various applications, such as finding the source of an image, identifying similar products, and detecting fake news.\n\nFace recognition / identification:\n\nFace recognition, also known as face identification, is a computer vision task that involves identifying a person from a digital image or a video frame. Face recognition can be done by extracting visual features, such as facial landmarks and texture, from the input image and comparing them to a database of known faces. Deep neural networks, such as Siamese networks and convolutional neural networks (CNNs), have been shown to achieve state-of-the-art performance on face recognition tasks.\n\nSiamese network:\n\nA Siamese network is a deep neural network architecture that consists of two identical subnetworks that share the same weights. Siamese networks are often used for tasks that involve measuring similarity or dissimilarity between pairs of input data, such as image matching, face recognition, and text similarity. Siamese networks work by feeding each input through the two subnetworks and computing a similarity score based on the distance between their output embeddings. Siamese networks can be trained using a contrastive loss function, which encourages similar inputs to have similar embeddings and dissimilar inputs to have dissimilar embeddings.","metadata":{}},{"cell_type":"markdown","source":"**Type I and II errors\nROC AUC\nEvaluation of models\nVideo processing pipeline**","metadata":{}},{"cell_type":"markdown","source":"Type I and II errors:\n\nType I and Type II errors are two types of errors that can occur when testing a hypothesis or making a decision based on a statistical model. A Type I error occurs when a null hypothesis is rejected even though it is true, while a Type II error occurs when a null hypothesis is not rejected even though it is false. In other words, a Type I error is a false positive, while a Type II error is a false negative.\n\nROC AUC:\n\nROC AUC, or Receiver Operating Characteristic Area Under the Curve, is a metric used to evaluate the performance of binary classification models. The ROC curve is a plot of the true positive rate against the false positive rate at various classification thresholds. The AUC score represents the area under the ROC curve, which measures the overall performance of the model in distinguishing between positive and negative examples. A higher AUC score indicates better performance, with a perfect classifier having an AUC score of 1.\n\nEvaluation of models:\n\nThe evaluation of models is an essential step in the machine learning pipeline. There are several metrics used to evaluate the performance of a model, depending on the specific task and the type of model. For classification tasks, metrics such as accuracy, precision, recall, and F1 score can be used. For regression tasks, metrics such as mean squared error (MSE), mean absolute error (MAE), and R-squared can be used. In addition to these metrics, cross-validation and hyperparameter tuning can also be used to assess the generalization and optimize the performance of the model.\n\nVideo processing pipeline:\n\nThe video processing pipeline refers to the series of steps involved in processing and analyzing video data. The pipeline typically consists of four main stages: acquisition, preprocessing, analysis, and output. In the acquisition stage, the video data is captured from a camera or a video file. In the preprocessing stage, the video data is preprocessed to remove noise, stabilize the image, and normalize the lighting conditions. In the analysis stage, various computer vision techniques, such as object detection, tracking, and recognition, are used to extract information from the video data. In the output stage, the results of the analysis are presented to the user in a meaningful way, such as a visualization or a report. The video processing pipeline can be applied to various applications, such as surveillance, robotics, and sports analysis.","metadata":{}},{"cell_type":"markdown","source":"**Object detection\nBounding box regression\nRCNN family\nMean average precision**","metadata":{}},{"cell_type":"markdown","source":"Object detection:\n\nObject detection is a computer vision task that involves detecting and localizing objects within an image or a video. The goal of object detection is to identify the objects of interest within an image and provide information about their location and size.\n\nBounding box regression:\n\nBounding box regression is a technique used in object detection to refine the location and size of bounding boxes around objects in an image. In this technique, a regression model is trained to predict the offsets between the initial bounding boxes and the true bounding boxes of the objects. The predicted offsets are then used to refine the initial bounding boxes and obtain more accurate locations and sizes of the objects.\n\nRCNN family:\n\nRCNN, or Region-based Convolutional Neural Network, is a family of object detection models that use a region-based approach to detect and localize objects in an image. The family includes several variants, such as Fast R-CNN, Faster R-CNN, and Mask R-CNN. These models use a combination of convolutional neural networks (CNNs) and region proposal algorithms to identify regions of interest in an image and then classify and refine them to obtain the final object detection results.\n\nMean Average Precision:\n\nMean Average Precision (mAP) is a commonly used metric for evaluating the performance of object detection models. It measures the average precision across all object classes and is calculated as the mean of the average precision scores for each class. The average precision score for each class is calculated as the area under the precision-recall curve for that class. A higher mAP score indicates better performance of the object detection model.","metadata":{}},{"cell_type":"markdown","source":"**YOLOv1**","metadata":{}},{"cell_type":"markdown","source":"YOLOv1, or You Only Look Once version 1, is a real-time object detection system introduced by Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi in 2015. It is a single-stage detector that uses a neural network to directly predict bounding boxes and class probabilities for objects in an image, without requiring a separate region proposal stage.\n\nThe YOLOv1 architecture consists of a single convolutional neural network (CNN) that processes the entire image at once and outputs a grid of predictions, each containing information about the bounding box and class of an object. The network divides the image into a grid of cells and predicts bounding boxes relative to each cell, as well as a confidence score indicating the probability that an object is present in the box. The final predictions are obtained by combining the predictions from multiple scales of the network.\n\nYOLOv1 was designed for real-time object detection in a variety of applications, such as surveillance, robotics, and self-driving cars. It achieved high detection accuracy and real-time performance, making it a popular choice for many computer vision tasks. However, it has since been surpassed by newer versions of YOLO and other object detection models, such as Faster R-CNN and Mask R-CNN, which offer improved accuracy and performance.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}